## [On-device Panoptic Segmentation for Camera Using Transformers](https://machinelearning.apple.com/research/panoptic-segmentation)
**Apple, Inc.**

**Summary**: This blog introduced a method for on-device (iPhone) [panoptic segmentation](https://arxiv.org/abs/1801.00868), which poses the constraints of computes and power. The proposed method combines the [Detection Transformer, or DETR](https://arxiv.org/abs/2005.12872), and [HyperNetworks](https://arxiv.org/abs/1609.09106) to generate segmentation output, while limiting the computation. DETR is a method for object detection, which lacks the power of generating pixel-level output, required for panoptic segmentation. Therefore, a convolutional decoder is used to produce pixel-level labels for the segmentation task (see diagram below, upper half). However, naively adding the decoder will brings more computation in the loop, which is undesired. The idea of using a network to producing dynamic weights during inference time, aka HyperNetworks, is adopted here to turn the Transformer into dynamic weight generator, as a meta-learning approach. The convolutional decoder output and the transformer output are convolved to produce the final segmentation masks. This final step is named dynamic convolution (see diagram below, lower half). The advantages of HyperDETR are two-fold: 1) reduced complexity (as the ROIs are not directly tied with the mask generation step), and 2) "scene-level categories, such as sky, can be handled purely through the convolutional path, which would let us skip the execution of the Transformer module when subject-level elements are not requested" (not sure what it means).

For implementation, MobileNetV3 (the large variant) acts as the convolutional encoder, while a decoder similar to U-Net is used. The model is also quantized after training. MobileNetV3 backbone is pre-trained on Apple's internal dataset of ~4M images from 1,500 categories. Segmentation modules (convolutional decoder and Transformer) are trained on ~50k images with high-quality alpha-matte annotations (people, skin, hair, glasses, teeth, and sky). Standard data augmentation techniques are used. The authors also attempted structured pruning to reduce model size but failed. What works better is a simple removal of last 4 layers out of the 6-layer transformer module.

tl;dr: A concrete work on panoptic segmentation for specific mobile application, which is likely productionized into iPhones (region highlighting when long-pressing on photos). Transformer is still only small part. Similar to [#SAM](https://arxiv.org/abs/2304.02643).

![image](https://mlr.cdn-apple.com/media/Fig_2_arch_black_ce86567d2f.png)
